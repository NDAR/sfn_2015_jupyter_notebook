{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using a miNDAR Package to Drive Exploration and Analysis\n",
    "\n",
    "1. Connecting to a miNDAR containing Omics data from an NDAR Study\n",
    "2. Querying data in the miNDAR\n",
    "3. Exploring data in the miNDAR\n",
    "4. Generating credentials for accessing s3 Objects\n",
    "5. Using credentials and s3 Object references from the miNDAR to retreive data\n",
    "6. Analyze data from an s3 Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Connecting to a miNDAR containing Omics data from an NDAR Study\n",
    "\n",
    "A Study Data Package can be generated by clicking the 'Download' button on a study from the [data from papers](https://ndar.nih.gov/data_from_papers.html).\n",
    "Once packaged, these data can be pushed to a miNDAR in the form of where they will be availalbe as database tables and s3 object references.\n",
    "When data has finished loading into a miNDAR, you will receive an email with the connection information.\n",
    "\n",
    "- requires a Oracle client\n",
    "- the following example uses [Oracle Instant Client](http://www.oracle.com/technetwork/topics/linuxx86-64soft-092277.html) v11.2 and the python library cx_Oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import cx_Oracle\n",
    "hostname = 'mindarvpc.cqahbwk3l1mb.us-east-1.rds.amazonaws.com'\n",
    "port = 1521\n",
    "sid = 'ORCL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "username = input('Enter your miNDAR username:')#'username_package_id'\n",
    "password = getpass.getpass('Enter your miNDAR password:')\n",
    "dsnStr = cx_Oracle.makedsn(hostname, port, sid )\n",
    "db = cx_Oracle.connect(user=username, password=password, dsn=dsnStr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Querying Data in the miNDAR\n",
    "\n",
    "After negotiating a successful connection we can query the miNDAR directly using SQL.  We can retreive these results and store them as objects in R, Python, and many other programming languages.\n",
    "\n",
    "Here we will query the table CONCEPT_BY_GUID, which is available in all miNDAR packages.\n",
    "\n",
    "- in this example we store the results in a pandas dataframe (requries [pandas](http://pandas.pydata.org/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas.io.sql as psql\n",
    "\n",
    "c = db.cursor()\n",
    "query=\"\"\"SELECT \n",
    "           concept_id,\n",
    "           subjectkey,\n",
    "           interview_Age,\n",
    "           gender, \n",
    "           short_name, \n",
    "           concept_hierarchy \n",
    "         FROM CONCEPT_BY_GUID\"\"\"\n",
    "c.execute(query)\n",
    "concept_by_guid = pd.DataFrame(c.fetchall())\n",
    "concept_by_guid.columns = [rec[0] for rec in c.description]\n",
    "c.close()\n",
    "print(concept_by_guid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally, here we are going to retreive data from the genomics_subject02 and genomcis_sample03 tables, which contain demographic and phenotypic data, and data on samples collected with file references for any data generated from these samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "query2 = \"\"\"SELECT DISTINCT samp.SUBJECTKEY, subj.PHENOTYPE, samp.DATA_FILE1, samp.DATA_FILE2\n",
    "           FROM GENOMICS_SAMPLE03 samp\n",
    "           JOIN genomics_subject02 subj on samp.subjectkey=subj.subjectkey\n",
    "           JOIN concept_by_guid conc on samp.subjectkey=conc.subjectkey\n",
    "           WHERE conc.CONCEPT_HIERARCHY like '%IQ\\\\\\\\Low%'\"\"\"\n",
    "c = db.cursor()\n",
    "c.execute(query2)\n",
    "data_files = pd.DataFrame(c.fetchall())\n",
    "data_files.columns = [rec[0] for rec in c.description]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(data_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Generating credentials for accessing s3 Objects\n",
    "\n",
    "Here we will generate credentials using our NIMH Data Archives username and password, which will allow us to authenticate and retreive data from AWS s3 Object storage for any shared objects.\n",
    "\n",
    "- We will use a python package for generating tokens, which will be available on [GitHub](https://github.com/NDAR) after this webinar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "url = 'https://ndar.nih.gov/DataManager/dataManager'\n",
    "username = input('Enter your NIMH Data Archives username:')\n",
    "password = getpass.getpass('Enter your NIMH Data Archives password:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from nda_aws_token_generator import *\n",
    "generator = NDATokenGenerator(url)\n",
    "token = generator.generate_token(username, password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print('aws_access_key_id=%s\\n'\n",
    "      'aws_secret_access_key=%s\\n'\n",
    "      'security_token=%s\\n'\n",
    "      'expiration=%s\\n' \n",
    "      %(token.access_key,\n",
    "        token.secret_key,\n",
    "        token.session,\n",
    "        token.expiration)\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Using credentials and s3 Object references from the miNDAR to retreive data\n",
    "\n",
    "Here we will combine our s3 credentials and the data we retrieved from the miNDAR, using the DATA_FILE2 column that has our s3 Object references to retreive some Variant Call data about a subject.\n",
    "\n",
    "- We will use [boto](https://github.com/boto/boto) to interface with s3 objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import boto.s3.connection\n",
    "cf = boto.s3.connection.OrdinaryCallingFormat()\n",
    "conn = boto.connect_s3( token.access_key, \n",
    "                        token.secret_key,\n",
    "                        security_token=token.session,\n",
    "                        calling_format=cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "print(data_files.ix[1,'DATA_FILE2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "bucket = urlparse(data_files.ix[1,'DATA_FILE2']).netloc\n",
    "key = urlparse(data_files.ix[1,'DATA_FILE2']).path\n",
    "bucket_object = conn.get_bucket(bucket)\n",
    "s3_object = boto.s3.key.Key(bucket_object)\n",
    "s3_object.key = key\n",
    "byte_data = s3_object.get_contents_as_string(headers={'Range': 'bytes=0-30000'})\n",
    "#print(byte_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(byte_data.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5. Analyze/Query data from an s3 Object\n",
    "\n",
    "Here we will retreive multiple VCF files and search them for a specific gene of interest. This implementation uses a simple text search, which works for the annotated VCF files in this collection because SnpEff has been run, and the ID field contains a SNPEFF entry that includes gene name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "import time\n",
    "import re\n",
    "from colorama import init, Fore, Back\n",
    "import re\n",
    "\n",
    "def get_vcf(s3_ref):\n",
    "    bucket = urlparse(s3_ref).netloc\n",
    "    key = urlparse(s3_ref).path\n",
    "    bucket_object = conn.get_bucket(bucket)\n",
    "    s3_object = boto.s3.key.Key(bucket_object)\n",
    "    s3_object.key = key\n",
    "    byte_data = s3_object.get_contents_as_string()\n",
    "    return byte_data.decode('utf-8')\n",
    "\n",
    "gene_name = input('Enter the name of the gene: ')\n",
    "for i in range(1,5): # Iterate over files 1-5; use data_files.index to iterate over all files in column.\n",
    "    print('Streaming data from s3 Object [%s] to memory...' % data_files.ix[i, 'DATA_FILE2'])\n",
    "    vcf_file = get_vcf(data_files.ix[i, 'DATA_FILE2'])\n",
    "    print('Searching file contents for gene [%s]\\r\\n' % gene_name)\n",
    "    count = 0\n",
    "    for line in vcf_file.rstrip().split('\\n'):\n",
    "        if gene_name in line:\n",
    "            count +=1\n",
    "            init() # only necessary on Windows          \n",
    "            print (re.sub(r'(.*)(' + re.escape(gene_name) +r')(.*)', r'\\1' + Fore.MAGENTA + r'\\2' + Fore.RESET + r'\\3', line))        \n",
    "    print('Found a total of [%s] variants annotated with this gene in the VCF file' % count)\n",
    "    print('\\r\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will retreive multiple VCF files and use the PyVCF python package, returning results from specific region. This example demonstrates using the in-memory file stream as a file-like object with the PyVCF library to access specific information contianed in the VCF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "import time\n",
    "import re\n",
    "from colorama import init, Fore, Back\n",
    "import re\n",
    "from io import StringIO\n",
    "import vcf\n",
    "\n",
    "def get_vcf(s3_ref):\n",
    "    bucket = urlparse(s3_ref).netloc\n",
    "    key = urlparse(s3_ref).path\n",
    "    bucket_object = conn.get_bucket(bucket)\n",
    "    s3_object = boto.s3.key.Key(bucket_object)\n",
    "    s3_object.key = key\n",
    "    byte_data = s3_object.get_contents_as_string()\n",
    "    return byte_data.decode('utf-8')\n",
    "\n",
    "print('Position-based query')\n",
    "chrom = int(input('Enter the chromosome   : '))\n",
    "start = int(input('Enter the Start Postion: '))\n",
    "end   = int(input('Enter the End Position: '))\n",
    "for i in range(1,5): # Iterate over files 1-5; use data_files.index to iterate over all files in column.\n",
    "    print('Streaming data from s3 Object [%s] to memory...' % data_files.ix[i, 'DATA_FILE1'])\n",
    "    vcf_file = StringIO(get_vcf(data_files.ix[i, 'DATA_FILE1']))\n",
    "    vcf_reader = vcf.Reader(vcf_file)\n",
    "    #vcf_reader.fetch(chrom, start, end)\n",
    "    print('Matching records:')\n",
    "    for record in vcf_reader:\n",
    "        #print(record)\n",
    "        if str(record.CHROM) == str(chrom) and (record.POS >= start and record.POS <= end):\n",
    "            for sample in record.samples:\n",
    "                call = record.genotype(sample.sample)\n",
    "                if hasattr(call.data, 'PL'):\n",
    "                    print('Sample: %s - %s DP=%s PL=%s QA=%s QR=%s RO=%s' % (sample, str(call.site),str(call.data.DP), str(call.data.PL), str(call.data.QA), str(call.data.QR), str(call.data.RO)))\n",
    "                else:\n",
    "                    print('Sample: %s - %s DP=%s QA=%s QR=%s RO=%s' % (sample, str(call.site),str(call.data.DP), str(call.data.QA), str(call.data.QR), str(call.data.RO)))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Query an indexed BAM file using samtools built with libcurl branch (supports authenticated access to secure AWS S3 objects)\n",
    "\n",
    "First we connect to a miNDAR that has file references for BAM files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "username = input('Enter your miNDAR username:')#'username_packageid for miNDAR with BAM files'\n",
    "password = getpass.getpass('Enter your miNDAR password:')\n",
    "dsnStr = cx_Oracle.makedsn(hostname, port, sid )\n",
    "db = cx_Oracle.connect(user=username, password=password, dsn=dsnStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = db.cursor()\n",
    "c.execute(query2)\n",
    "data_files = pd.DataFrame(c.fetchall())\n",
    "data_files.columns = [rec[0] for rec in c.description]\n",
    "print(data_files.ix[1,'DATA_FILE1'])\n",
    "bam_file = data_files.ix[1, 'DATA_FILE1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After negotiating our connection and retreiving a file reference, wefollow the instructions to build the libcurl branch of htslib and samtools.\n",
    "\n",
    "https://www.biostars.org/p/147772/ -develop branch of samtools/htslib and samtools/samtools support access to private S3 objects\n",
    "\n",
    "Then we set our environment variables using our AWS Federated Tokeb.\n",
    "\n",
    "Finally we run samtools to view a specific region of Chromosome 20 of the indexed BAM file in s3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#https://www.biostars.org/p/147772/ -develop branch of samtools/htslib and samtools/samtools support access to private S3 objects\n",
    "#https://github.com/samtools/htslib/pull/232 Plan to merge this support into official 1.3 release of htslib/samtools\n",
    "import os\n",
    "os.environ['AWS_ACCESS_KEY_ID']=token.access_key\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=token.secret_key\n",
    "os.environ['AWS_SESSION_TOKEN']=security_token=token.session\n",
    "!samtools view $bam_file 20:1000-100000 "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
